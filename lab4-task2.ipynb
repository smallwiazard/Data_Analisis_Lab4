{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T13:29:35.479611Z","iopub.execute_input":"2023-01-18T13:29:35.481226Z","iopub.status.idle":"2023-01-18T13:29:43.115262Z","shell.execute_reply.started":"2023-01-18T13:29:35.481067Z","shell.execute_reply":"2023-01-18T13:29:43.114122Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"text_file = \"/kaggle/input/polishenglish-dataset/pol.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:43.117013Z","iopub.execute_input":"2023-01-18T13:29:43.118333Z","iopub.status.idle":"2023-01-18T13:29:43.124781Z","shell.execute_reply.started":"2023-01-18T13:29:43.118287Z","shell.execute_reply":"2023-01-18T13:29:43.123441Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open(text_file) as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n    eng, pol, smth = line.split(\"\\t\")\n    pol = \"[start] \" + pol + \" [end]\"\n    text_pairs.append((eng, pol))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:43.126301Z","iopub.execute_input":"2023-01-18T13:29:43.126735Z","iopub.status.idle":"2023-01-18T13:29:43.343736Z","shell.execute_reply.started":"2023-01-18T13:29:43.126699Z","shell.execute_reply":"2023-01-18T13:29:43.342387Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for _ in range(5):\n    print(random.choice(text_pairs))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:43.346380Z","iopub.execute_input":"2023-01-18T13:29:43.347247Z","iopub.status.idle":"2023-01-18T13:29:43.354599Z","shell.execute_reply.started":"2023-01-18T13:29:43.347205Z","shell.execute_reply":"2023-01-18T13:29:43.353148Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"('I miss you. I need to see you. Could I come over?', '[start] Tęsknię za tobą. Chciałbym się z tobą zobaczyć. Mogę przyjść? [end]')\n('Instead of coming directly home, I took the long way and stopped by the post office.', '[start] Zamiast iść prosto do domu, poszedłem dłuższą drogą i zatrzymałem się na poczcie. [end]')\n('He saved a lot of money.', '[start] Zaoszczędził sporo pieniędzy. [end]')\n('I wonder why Tom killed himself.', '[start] Zastanawiam się, dlaczego Tom się zabił. [end]')\n('Tom likes his new job.', '[start] Tom lubi swoją nową pracę. [end]')\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:43.356938Z","iopub.execute_input":"2023-01-18T13:29:43.357588Z","iopub.status.idle":"2023-01-18T13:29:43.421116Z","shell.execute_reply.started":"2023-01-18T13:29:43.357494Z","shell.execute_reply":"2023-01-18T13:29:43.419791Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"46424 total pairs\n32498 training pairs\n6963 validation pairs\n6963 test pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"strip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\nvocab_size = 15000\nsequence_length = 20\nbatch_size = 64\n\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\npol_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)\ntrain_eng_texts = [pair[0] for pair in train_pairs]\ntrain_pol_texts = [pair[1] for pair in train_pairs]\neng_vectorization.adapt(train_eng_texts)\npol_vectorization.adapt(train_pol_texts)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:43.422665Z","iopub.execute_input":"2023-01-18T13:29:43.423326Z","iopub.status.idle":"2023-01-18T13:29:46.242614Z","shell.execute_reply.started":"2023-01-18T13:29:43.423273Z","shell.execute_reply":"2023-01-18T13:29:46.241256Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2023-01-18 13:29:43.481724: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2023-01-18 13:29:44.203802: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"def format_dataset(eng, pol):\n    eng = eng_vectorization(eng)\n    pol = pol_vectorization(pol)\n    return ({\"encoder_inputs\": eng, \"decoder_inputs\": pol[:, :-1],}, pol[:, 1:])\n\n\ndef make_dataset(pairs):\n    eng_texts, pol_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    pol_texts = list(pol_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, pol_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:46.244417Z","iopub.execute_input":"2023-01-18T13:29:46.244819Z","iopub.status.idle":"2023-01-18T13:29:46.829537Z","shell.execute_reply.started":"2023-01-18T13:29:46.244784Z","shell.execute_reply":"2023-01-18T13:29:46.828092Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:46.831365Z","iopub.execute_input":"2023-01-18T13:29:46.831778Z","iopub.status.idle":"2023-01-18T13:29:47.486069Z","shell.execute_reply.started":"2023-01-18T13:29:46.831741Z","shell.execute_reply":"2023-01-18T13:29:47.484776Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"inputs[\"encoder_inputs\"].shape: (64, 20)\ninputs[\"decoder_inputs\"].shape: (64, 20)\ntargets.shape: (64, 20)\n","output_type":"stream"},{"name":"stderr","text":"2023-01-18 13:29:47.223382: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:47.488441Z","iopub.execute_input":"2023-01-18T13:29:47.488840Z","iopub.status.idle":"2023-01-18T13:29:47.514823Z","shell.execute_reply.started":"2023-01-18T13:29:47.488804Z","shell.execute_reply":"2023-01-18T13:29:47.513078Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"embed_dim = 256\nlatent_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:47.519643Z","iopub.execute_input":"2023-01-18T13:29:47.521149Z","iopub.status.idle":"2023-01-18T13:29:48.848923Z","shell.execute_reply.started":"2023-01-18T13:29:47.521096Z","shell.execute_reply":"2023-01-18T13:29:48.847528Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"epochs = 3  # This should be at least 30 for convergence\n\ntransformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\ntransformer.fit(train_ds, epochs=epochs, validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:29:48.851114Z","iopub.execute_input":"2023-01-18T13:29:48.851676Z","iopub.status.idle":"2023-01-18T14:07:02.784428Z","shell.execute_reply.started":"2023-01-18T13:29:48.851625Z","shell.execute_reply":"2023-01-18T14:07:02.783051Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"transformer\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nencoder_inputs (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\npositional_embedding (Positiona (None, None, 256)    3845120     encoder_inputs[0][0]             \n__________________________________________________________________________________________________\ndecoder_inputs (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntransformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n__________________________________________________________________________________________________\nmodel_1 (Functional)            (None, None, 15000)  12959640    decoder_inputs[0][0]             \n                                                                 transformer_encoder[0][0]        \n==================================================================================================\nTotal params: 19,960,216\nTrainable params: 19,960,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/3\n508/508 [==============================] - 750s 1s/step - loss: 1.7509 - accuracy: 0.3611 - val_loss: 1.5500 - val_accuracy: 0.4134\nEpoch 2/3\n508/508 [==============================] - 736s 1s/step - loss: 1.5443 - accuracy: 0.4244 - val_loss: 1.4103 - val_accuracy: 0.4548\nEpoch 3/3\n508/508 [==============================] - 742s 1s/step - loss: 1.4471 - accuracy: 0.4564 - val_loss: 1.2908 - val_accuracy: 0.4799\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f578ccd1590>"},"metadata":{}}]},{"cell_type":"code","source":"pol_vocab = pol_vectorization.get_vocabulary()\npol_index_lookup = dict(zip(range(len(pol_vocab)), pol_vocab))\nmax_decoded_sentence_length = 20\n\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = pol_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = pol_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(3):\n    input_sentence = random.choice(test_eng_texts)\n    translated = decode_sequence(input_sentence)\n    print(input_sentence, \"\\t\", translated)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:11:39.406118Z","iopub.execute_input":"2023-01-18T14:11:39.406848Z","iopub.status.idle":"2023-01-18T14:11:40.739324Z","shell.execute_reply.started":"2023-01-18T14:11:39.406789Z","shell.execute_reply":"2023-01-18T14:11:40.738307Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"These are gifts. \t [start] to jest [UNK] [end]\nI don't understand what you're trying to say. \t [start] nie wiem co co się ci zrobić [end]\nI'm studying French and web design. \t [start] jestem w francusku i [UNK] [end]\n","output_type":"stream"}]}]}